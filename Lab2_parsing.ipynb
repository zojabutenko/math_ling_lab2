{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.5.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from bs4) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала заходим в архив и генерируем ссылки на публикации каждого дня с начала 2011 года (появление архива) по конец 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for y in range(10):\n",
    "    y = 2011 + y\n",
    "    for m in range(1, 13):\n",
    "        for d in range(1, 32):\n",
    "            url = 'http://www.vpravda.ru/archive/' + str(y) + '-' + str(m) + '-' + str(d) + '/'\n",
    "            urls.append(url)\n",
    "# print(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заходим по каждой из сгенерированных ссылок и собираем ссылки на конкретные статьи за этот день. Если статей в этот день не было, ссылка не добавляется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/obshchestvo/artrit-mozhet-vliyat-na-prodolzhitelnost-zhizni-97976/', '/sport/mezhdu-nebom-i-zemley-107/', '/obshchestvo/otchet-ot-pervogo-lica-215/', '/obshchestvo/esli-ne-povernemsya-licom-k-vrachu-nikakaya-modernizaciya-ne-pomozhet-282/', '/obshchestvo/glavnaya-figura-potrebitel-281/', '/ekonomika/glavnaya-figura-potrebitel-307/', '/kultura/zhil-na-zemle-hudozhnik-429/', '/obshchestvo/nachalo-peremen-1634/', '/obshchestvo/ya-moroza-ne-boyus-s-golovoyu-okunus-1693/', '/obshchestvo/novyy-ustav-regiona-byl-prorabotan-i-prinyat-v-rekordnye-sroki-1933/']\n"
     ]
    }
   ],
   "source": [
    "def __get_page_content(url):\n",
    "    session = requests.Session()\n",
    "    return session.get(url).content.decode('ascii', errors='ignore')\n",
    "\n",
    "texts_links = []\n",
    "for u in range(len(urls)):\n",
    "    file = __get_page_content(urls[u])\n",
    "    soup = BeautifulSoup(file, 'lxml')\n",
    "    tag2 = soup.select_one(\"aside\", class_='sidebars')\n",
    "    tag2.decompose()\n",
    "    tag = soup.find('div', id='main')\n",
    "    tag = tag.div\n",
    "    tag = tag.div\n",
    "    tag = tag.div\n",
    "    if tag is not None:\n",
    "        tags = tag.find_all('div', class_='field-content')\n",
    "        for t in tags:\n",
    "            t = t.a\n",
    "            link = str(t)\n",
    "            res = re.search(r'href=\"(.+)\">', link)\n",
    "            if res is not None:\n",
    "                the_link = res.group(1)\n",
    "                if the_link not in texts_links:\n",
    "                    texts_links.append(the_link)\n",
    "print(texts_links[:10])\n",
    "#             link = link.find('a')\n",
    "#             if link.has_attr('href'):\n",
    "#                 texts_links.append(link['href'])\n",
    "#                 print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходимся по списку всех статей, которые когда-либо были опубликованы на сайте Волгоградской правды, парсим код страницы и вытаскиваем из него автора, дату, заголовок, подзаголовок, текст. Добавляем источник и ссылку, сохраняем в нужном виде в один текстовый файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_page_content(url):\n",
    "    session = requests.Session()\n",
    "    r = session.get(url)\n",
    "    r.encoding = 'utf-8'\n",
    "    return r\n",
    "\n",
    "with open('full_texts.txt', 'w', encoding='utf-8') as ff:\n",
    "    ff.write('')\n",
    "    \n",
    "for text in texts_links:\n",
    "    actual_link = 'https://www.vpravda.ru/' + text\n",
    "    page = __get_page_content(actual_link)\n",
    "    with open('full_texts.txt', 'a', encoding='utf-8') as f:\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.text, 'lxml', from_encoding='utf-8')\n",
    "            tag = soup.find('div', id='main')\n",
    "            if tag:\n",
    "                tag = tag.find('div', id='content')\n",
    "                title = tag.h1.text\n",
    "                tag = tag.article\n",
    "                date = tag.find('div', class_='field field-name-field-article-date field-type-date field-label-hidden')\n",
    "                if date:\n",
    "                    date = date.div\n",
    "                    date = date.div\n",
    "                    date = date.span.text\n",
    "                author = tag.find('div', class_='field field-name-field-article-author field-type-text field-label-hidden')\n",
    "                if author:\n",
    "                    author = author.div\n",
    "                    author = author.div.text\n",
    "\n",
    "                text_main = tag.find('div', class_='field field-name-body field-type-text-with-summary field-label-hidden')\n",
    "                if text_main:\n",
    "                    text_main = text_main.div\n",
    "                    text_undertitle = tag.find('div', class_='field field-name-field-article-lead field-type-text-long field-label-hidden')\n",
    "                    if text_undertitle:\n",
    "                        text_undertitle = text_undertitle.div\n",
    "                        text_undertitle = str(text_undertitle.div.text)\n",
    "                        text_main = str(text_main.div.text)\n",
    "                        text = re.sub('\\n', '', text)\n",
    "                        text = text_undertitle + text_main\n",
    "                    else:\n",
    "                        text = text_main\n",
    "                    f.write('=====\\n' + actual_link + '\\n' + 'Волгоградская правда' + '\\n' + str(date)[:10] + '\\n' + str(author) + '\\n' + str(title) + '\\n' + str(text) + '\\n')\n",
    "        except ConnectionResetError:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге у нас получается файл full_texts.txt, в котором записаны подряд все статьи в хронологическом порядке. Этот файл мы будем использовать как корпус в lab2_graphs.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
